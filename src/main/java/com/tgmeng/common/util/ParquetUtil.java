package com.tgmeng.common.util;

import com.tgmeng.common.bean.HotPointDataParquetBean;
import com.tgmeng.common.parquet.HotPointDataParquetSchema;
import lombok.extern.slf4j.Slf4j;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.parquet.avro.AvroParquetWriter;
import org.apache.parquet.hadoop.ParquetWriter;
import org.apache.parquet.hadoop.metadata.CompressionCodecName;

import java.io.File;
import java.util.List;

@Slf4j
public class ParquetUtil {

    static {
        try {
            // è·å–é¡¹ç›®æ ¹ç›®å½•
            String projectDir = System.getProperty("user.dir");
            File hadoopDir = new File(projectDir, ".hadoop");
            File binDir = new File(hadoopDir, "bin");
            // åˆ›å»ºç›®å½•
            binDir.mkdirs();
            // è®¾ç½® Hadoop Home
            System.setProperty("hadoop.home.dir", hadoopDir.getAbsolutePath());
            System.out.println("âœ… Hadoop ç›®å½•: " + hadoopDir.getAbsolutePath());
        } catch (Exception e) {
            System.err.println("âš ï¸ åˆå§‹åŒ– Hadoop ç¯å¢ƒå¤±è´¥: " + e.getMessage());
        }
    }

    private final Schema schema;
    private final Configuration conf;

    public ParquetUtil() {
        System.out.println("ğŸ“ å¼€å§‹åˆå§‹åŒ– HotPointDataParquetUtil");

        this.schema = HotPointDataParquetSchema.getSchema();
        System.out.println("ğŸ“‹ Schema: " + schema);

        this.conf = new Configuration();
        conf.set("fs.file.impl", org.apache.hadoop.fs.LocalFileSystem.class.getName());
        conf.setBoolean("dfs.permissions.enabled", false);
        conf.set("fs.permissions.umask-mode", "000");

        System.out.println("âœ… HotPointDataParquetUtil åˆå§‹åŒ–å®Œæˆ");
    }

    // å†™å…¥æœ¬åœ°
    public void writeParquet(List<HotPointDataParquetBean> records, String outputPath) throws Exception {
        writeParquetWithConf(records, outputPath, conf);
    }

    // é€šç”¨å†™å…¥æ–¹æ³•
    private void writeParquetWithConf(List<HotPointDataParquetBean> records,
                                      String outputPath,
                                      Configuration configuration) throws Exception {
        Path path = new Path(outputPath);

        try (ParquetWriter<GenericRecord> writer = AvroParquetWriter
                .<GenericRecord>builder(path)
                .withSchema(schema)
                .withConf(configuration)  // ä½¿ç”¨ä¼ å…¥çš„é…ç½®
                .withCompressionCodec(CompressionCodecName.ZSTD)
                .withDictionaryEncoding(true)
                .withRowGroupSize(128 * 1024 * 1024)
                .withPageSize(1024 * 1024)
                .build()) {

            for (HotPointDataParquetBean record : records) {
                GenericRecord avroRecord = convertToAvroRecord(record);
                writer.write(avroRecord);
            }

            System.out.println("âœ… æˆåŠŸå†™å…¥ " + records.size() + " æ¡è®°å½•åˆ° " + outputPath);
        }
    }

    private GenericRecord convertToAvroRecord(HotPointDataParquetBean record) {
        GenericRecord avroRecord = new GenericData.Record(schema);
        avroRecord.put("url", record.getUrl());
        avroRecord.put("title", record.getTitle());
        avroRecord.put("platformName", record.getPlatformName());
        avroRecord.put("platformCategory", record.getPlatformCategory());
        avroRecord.put("dataUpdateTime", record.getDataUpdateTime());
        avroRecord.put("simHash", record.getSimHash());
        return avroRecord;
    }
}